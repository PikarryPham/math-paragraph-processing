{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "18127022.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CQ9sqY1p2bmy",
        "cDluCJLQRIOk",
        "hYYvDqZATMk2",
        "UCcX5HsfW0Bk"
      ],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWfL5EzWztxD",
        "colab_type": "text"
      },
      "source": [
        "# LAB05 - PROJECT4 REPORT: MÔ HÌNH HỒI QUY TUYẾN TÍNH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIsFQHNB1nfC",
        "colab_type": "text"
      },
      "source": [
        "* **Họ và tên**: Phạm Ngọc Thùy Trang - MSSV: 18127022\n",
        "* **Lớp**: 18CLC1 - Môn: Toán Ứng dụng và thống kê\n",
        "* **Giáo Viên Hướng Dẫn:** Trần Thị Thảo Nhi\n",
        "* **Khoa**: Công Nghệ Thông Tin - HCMUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEnxIpbS2OFl",
        "colab_type": "text"
      },
      "source": [
        "1. Import các thư viện cần thiết (đã có sẵn trong file đề và không được phép thay đổi)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vggDMjMpz1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DON'T CHANGE this part: import libraries\n",
        "import numpy as np\n",
        "import scipy\n",
        "import json\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import re\n",
        "import itertools"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYuVivAW2g6x",
        "colab_type": "text"
      },
      "source": [
        "2. Đọc đường dẫn truyền vào tập dữ liệu gồm: tập train, tập valid và một con số random bất kì"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrN87-VTpz2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcafbf2c-0720-423e-b6be-2981d5a7bfff"
      },
      "source": [
        "# DON'T CHANGE this part: read data path\n",
        "train_set_path, valid_set_path, random_number = input().split()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.json valid.json 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ9sqY1p2bmy",
        "colab_type": "text"
      },
      "source": [
        "## I. XỬ LÝ DỮ LIỆU DẠNG VĂN BẢN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDnCJrou3NIx",
        "colab_type": "text"
      },
      "source": [
        "*Bài toán này em làm theo các bước trong TODO list dưới đây để đảm bảo ra được kết quả hoàn chỉnh nhất*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElMQH24Ypz2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:\n",
        "# 1. preprocess: converting text to lowercase, coverting number, tokenization, removing stopword, stemming\n",
        "# 2. embedding: hitogram matrix\n",
        "# 3. classifier using linear regression\n",
        "# 4. accuracy (for metric)\n",
        "# Em làm theo 4 bước ở đây"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzK8w2hk3knf",
        "colab_type": "text"
      },
      "source": [
        "* Đọc file dữ liệu gồm file valid và file train\n",
        "* Download các package \"punkt\" và \"stopwords\" trong thư viện nltk có sẵn để\n",
        "tiền xử lý dữ liệu dạng văn bản"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSXFRewbqsZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(train_set_path)\n",
        "#print(valid_set_path)\n",
        "#print(random_number)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPvoHDAxwV02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ef01e3b7-e277-418c-c458-863a333b59c8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0ts7sB3_fu",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm read_json(data_path)**: được dùng để đọc file json vì hai tập dữ liệu của chúng ta đều có dạng .json\n",
        "    - **Tham số đầu vào:** đường dẫn của tập dữ liệu\n",
        "    - **Tham số trả về:** danh sách các trường \"reviewText\" và các trường \"overall\" của từng object trong file\n",
        "\n",
        "* Sau đó lần lượt gán tập X_train, Y_train là hai giá trị được trả về từ hàm read_json (đối với tập train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgKs8WrPzKek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# đọc file json\n",
        "def read_json(data_path):\n",
        "    overall_list = []\n",
        "    review_list = []\n",
        "    f = open(data_path)\n",
        "    data = json.load(f) \n",
        "    for item in data: \n",
        "        overall_list.append(item['overall'])\n",
        "        review_list.append(item['reviewText'])\n",
        "    return review_list,overall_list\n",
        "\n",
        "\n",
        "X_train,y_train = read_json(train_set_path)\n",
        "\n",
        "#print(len(X_train),len(y_train))\n",
        "#print(X_train[0])\n",
        "#print(y_train[0])\n",
        "\n",
        "#Dữ liệu sau khi chạy hai hàm print ở trên cho tập train X và tập train Y\n",
        "# 5000 5000\n",
        "# I needed SHORT cables to connect adjacent units in a rack.  This was a perfect fit.  The colors make signal tracing a matter of sight rather than  scribble strips.\n",
        "# 5.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqjs7dmm5QmX",
        "colab_type": "text"
      },
      "source": [
        "* Gán tập X_valid, Y_valid tương ứng với kết quả trả về từ hàm read_file đối với tập dữ liệu valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr0axv0EzLgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_valid,y_valid = read_json(valid_set_path)\n",
        "\n",
        "#print(len(X_valid),len(y_valid))\n",
        "#print(X_valid[0])\n",
        "#print(y_valid[0])\n",
        "\n",
        "#--lấy ra giá trị X và giá trị Y thông qua hàm đọc file json\n",
        "# print(len(X_valid),len(y_valid)) -- 500 500\n",
        "# print(X_valid[0]) -- These pens never get old. I prefer medium point. LOVE the mulitple colors.\n",
        "# print(y_valid[0]) -- 5.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnqndkoNpz2I",
        "colab_type": "text"
      },
      "source": [
        "#### Ví dụ cho phần báo cáo, nên báo cáo cho từng phần code để rõ ràng\n",
        "\n",
        "Báo cáo phần tiền xử lý: dùng xyz để tách từ, ...\n",
        "\n",
        "... Đối với những từ out-of-vocab (xuất hiện trong tập train nhưng không có ở tập valid), xử lý bằng cách ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6IEfwSm5m2C",
        "colab_type": "text"
      },
      "source": [
        "### Qúa trình preprocess (tiền xử lý dữ liệu dạng văn bản)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMd6Cw1n50Qm",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm preprocess_train(text)**: dùng để tiền xử lý dạng văn bản, chuyển tất cả các chữ thành chữ thường, chuyển số thành ký tự \"num\" cho đơn giản, tách từ, loại bỏ stopwords *(các short word như the, is, at; các từ thường xuất hiện như what, this, how, các từ cực kỳ hiếm khi xuất hiện)*, stemming *(biến đổi một từ về dạng gốc, loại bỏ \"endings\" ra khỏi từ)*, chuyển các từ chưa xuất hiện trong quá trình học thành ký tự \"unk\"\n",
        "  1. **Tham số đầu vào:** văn bản chưa được xử lý (dạng thô)\n",
        "  2. **Tham số đầu ra:** văn bản đã được tiền xử lý bằng cách sử dụng các bước ở trên như mô tả"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJpRmguK8ZCQ",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm preprocess(X_data)**: truyền vào trong nó là một tập dữ liệu, hàm này được dùng để tiền xử lý cho tập train *(có thể xem thêm ở phần print(pre_text[int(random_number)])*. Thay vì trả về một danh sách gồm các đoạn văn trong tập train như ở trên, hàm này chỉ trả về một câu hay một đoạn văn bản đã được tiền xử lý dựa trên con số random mà ta nhập vào trước đó. Dan**h sách trả về là một list**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sSgLPCtpz2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ví dụ cho phần code hàm tiền xử lý tập train\n",
        "# 1. preprocess: converting text to lowercase, coverting number, tokenization, removing stopword, stemming\n",
        "# Đầu vào là 1 văn bản, đầu ra văn bản đã được tiền xử lý sử dụng các bước ở trên.\n",
        "stemmer= PorterStemmer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def preprocess_train(text):\n",
        "    # converting text to lowercase\n",
        "    text = text.lower()\n",
        "    # Chuyển giá trị số về num\n",
        "    text = re.sub(r\"[0-9]+\", \" num \", text)\n",
        "    # Tokenizer\n",
        "    tokens = word_tokenize(text)\n",
        "    # xoá stopword\n",
        "    text = [i for i in tokens if not i in stop_words]\n",
        "    # Stemming\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "def preprocess(X_data):\n",
        "    list_new = []\n",
        "    for x in X_data:\n",
        "        x_processed = preprocess_train(x)\n",
        "        list_new.append(x_processed)\n",
        "    return list_new\n",
        "\n",
        "pre_text = preprocess(X_train)\n",
        "#print(pre_text[int(random_number)])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYfFvDWpEJdc",
        "colab_type": "text"
      },
      "source": [
        "* Đoạn code này được dùng để lấy toàn bộ vocabulary trong tập train để kiểm tra trong tập test và thay bằng giá trị unk\n",
        "\n",
        "* Có thể chạy thử hai câu lệnh print(len(vocab)) và print(vocab) để biết được số lượng từ vựng trong tập train cũng như các từ trong tập train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iZUcLMN056r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lấy toàn bộ vocabulary trong tập train để kiếm tra trong tập test và thay bằng giá trị unk\n",
        "vocab = [\"unk\"]\n",
        "for item in pre_text:\n",
        "    for token in item:\n",
        "        if token not in vocab:\n",
        "            vocab.append(token)\n",
        "#print(\"Số lượng từ vựng trong tập train: \", len(vocab))\n",
        "#print(vocab): các từ trong tập train"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p-CO3WC14wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(X_valid) --['These pens never get old. I prefer medium point. LOVE the mulitple colors.', \"Not as strong as I would have liked as is my Peavey mike but they'll do for now for the price.\", 'Great calendar but the bottom edge of the sheets will wrinkle and fray with use. If there was a protective bottom edge it would make this a great product.',...]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4-4r8HLE00W",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm preprocess_testData(text,vocab)**: hàm này được dùng để xử lý trên tập valid *(tập test)*. Bởi vì đề có yêu cầu các từ nào không xuất hiện trong tập train thì thay bằng giá trị \"unk\". Do đó chúng ta chúng ta cần xây dựng 1 hàm riêng với đầu vào là 1 văn bản và tập từ điển trong tập train để kiểm tra xem cái từ trong tập test có tồn tại trong từ điển tập train không. Nếu tồn tại thì giữ nguyên. Không tồn tại thì thay bằng giá trị \"unk\". Qúa trình xử lý tiền văn bản tương tự như hàm preprocess_train.\n",
        "  - **Tham số đầu vào**: văn bản chưa được xử lý (dạng thô) và tập vocab\n",
        "  - **Tham số trả về:** văn bản đã được tiền xử lý bằng cách sử dụng các bước ở trên như mô tả\n",
        "\n",
        "* **Hàm preprocess(X_valid, vocab):** truyền vào trong nó là một tập dữ liệu và tập vocab, hàm này được dùng để tiền xử lý cho tập valid. Nếu như hàm trên được dùng để xử lý trên từng văn bản thì hàm này là duyệt toàn bộ dữ liệu trong tập valid/tập test. Danh sách trả về là một list.\n",
        "  - **Tham số đầu vào:** tập valid, tập vocab\n",
        "  - **Tham số trả về:** một list đã được xử lý. Từ đây ta có thể in ra câu thứ random_number trong tập valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8xlcmjnxvFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5c0828f2-ed17-4100-8b23-03e02d8b32b8"
      },
      "source": [
        "# 1. preprocess: converting text to lowercase, coverting number, tokenization, removing stopword, stemming\n",
        "def preprocess_testData(text,vocab):\n",
        "    # converting text to lowercase\n",
        "    text = text.lower()\n",
        "    # Chuyển giá trị số về num\n",
        "    text = re.sub(r\"[0-9]+\", \" num \", text)\n",
        "    # Tokenizer\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopword\n",
        "    text = [i for i in tokens if not i in stop_words]\n",
        "    # Stemming\n",
        "    text = [stemmer.stem(word) for word in text]\n",
        "    # chuyển chữ trong tập test thành giá trị unk\n",
        "    list_new = []\n",
        "    for token in text:\n",
        "        if token in vocab:\n",
        "            list_new.append(token)\n",
        "        else:\n",
        "            list_new.append(\"unk\")\n",
        "    text = list_new\n",
        "    return text\n",
        "\n",
        "# Hàm trên là xử lý ttừng văn bản. Còn hàm này là duyệt toàn bộ dữ liệu trong tập valid hoặc tập test.\n",
        "# Danh sách là trả về một list.\n",
        "def preprocess_test(X_valid,vocab): \n",
        "    list_process = []\n",
        "    for item in X_valid:\n",
        "        x_processed = preprocess_testData(item,vocab)\n",
        "        list_process.append(x_processed)\n",
        "    return list_process\n",
        "\n",
        "pre_text_valid = preprocess_test(X_valid,vocab)\n",
        "print(pre_text_valid[int(random_number)]) #in câu thứ random_number của tập valid (từ valid_set_path)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['legal', 'pad', 'throughout', 'offic', 'differ', 'note', '.', 'regular', 'legal', 'pad', 'get', 'job', 'done', 'top', 'gold', 'unk', 'pad', 'great', 'heavi', 'feel', 'sheet', 'nice', 'stock', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCoyuo3lJzt5",
        "colab_type": "text"
      },
      "source": [
        "### Xây dựng bộ ma trận nhúng của văn bản (document embedding): mỗi văn bản là 1 vector histogram (có chiều bằng chiều dài bộ từ điển vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ana7vZdM9qL",
        "colab_type": "text"
      },
      "source": [
        "Có nhiều cách để chúng ta xây dựng vector hóa văn bản, nhưng ở đây ta buộc phải dùng histogram vector\n",
        "\n",
        "* **Hàm histogram_vector(document, vocab)**: Với mỗi văn bản là một vector histogram có chiều dài bằng chiều dài bộ từ điển vocab và với ý nghĩa là mỗi thành phần i chứa tần suất xuất hiện của từ thứ i trong document đang xét *(có thể dùng câu lệnh histogram_vector(s,[\"i\",\"love\",\"baby\"]) để kiểm tra)*. Hàm này được xây dựng dựa trên công thức histogram vector (Chương Document Count Vector - Slide tham khảo của cô - trang 20)\n",
        "    - **Tham số đầu vào**: document bất kỳ, tập từ vựng vocab\n",
        "    - **Tham số trả về**: một vector histogram với chiều dài bằng chiều dài bộ từ điển vocab có chứa mỗi phần tử i là tần suất xuất hiện của từ thứ i trong document đang xét"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQWbVbDgrPhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# câu 2 . Histogram. Đầu vào là 1 văn bản, đầu ra là vector histogram. Document embedding đó.\n",
        "# Ta xem lại cái công thức histogram vector trong slide\n",
        "def histogram_vector(document,vocab):\n",
        "    vector = np.zeros(len(vocab))\n",
        "    vector_one = np.ones(len(vocab))\n",
        "    for index,word in enumerate(vocab):\n",
        "        if word in document:\n",
        "            vector[index] = document.count(word)\n",
        "    histogram = vector/(vector_one.T @ vector)\n",
        "    return histogram\n",
        "# Ví dụ\n",
        "s = \"i love you baby baby\".split(' ')\n",
        "#histogram_vector(s,[\"i\",\"love\",\"baby\"])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmH-WYo1aNuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(pre_text[0])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maF7F9I6PoQw",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm matrix_histogram(X_data, vocab)**: tính giá trị histogram cho toàn bộ dữ liệu và xây dựng bộ ma trận nhúng của văn bản. Có thể dùng câu lệnh print(histogram_X_train[0]) để hiển thị ví dụ document embedding hay câu lệnh print(len(histogram_X_train[0])) để hiển thị độ dài của một vector histogram để kiểm chứng\n",
        "    - **Tham số đầu vào:** danh sách các dữ liệu từng tập, bộ từ vựng vocab\n",
        "    - **Tham số đầu ra:** ma trận histogram document cho toàn bộ dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTid8Tzmsu46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tính giá trị histogram cho toàn bộ tập dữ liệu. Đầu vào là danh sách dữ liệu từng tập. Đầu ra là matrận histogram document cho toàn bộ dữ liệu\n",
        "def matrix_histogram(X_data,vocab):\n",
        "    matrix = []\n",
        "    for item in X_data:\n",
        "        matrix.append(histogram_vector(item,vocab))\n",
        "    return matrix\n",
        "\n",
        "histogram_X_train = matrix_histogram(pre_text,vocab)\n",
        "histogram_X_valid = matrix_histogram(pre_text_valid,vocab)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwXY-0kWZ3z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hiện thị ví dụ document embedding (histogram vector)\n",
        "# print(histogram_X_train[0]) --[0.         0.04761905 0.04761905 ... 0.         0.         0.        ]\n",
        "# Hiện thị độ dài của 1 vector hisgram. Nó là chiều của tập từ điển vocab. Xem video trình bày Cô sẽ hiểu cái.\n",
        "# print(len(histogram_X_train[0])) --6523"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDluCJLQRIOk",
        "colab_type": "text"
      },
      "source": [
        "### Chuẩn hóa nhãn thành dạng số hoặc vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQjkHAl9RMUR",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm category_vector(y)**: Được dùng để chuyển nhãn thành vector, đầu vào là một nhãn bất kỳ và đầu ra là một vector tương ứng\n",
        "\n",
        "* **Hàm convert_y(y_data):** Hàm này được dùng để chuyển toàn bộ nhãn y trong tập train và trong tập valid thành vector 5 chiều, và kết quả trả về tất nhiên là một ma trận. Ví dụ nhãn 1.0 sẽ có kết quả là [1,0,0,0,0], nhãn 5.0 sẽ có kết quả là [0,0,0,0,1]\n",
        "\n",
        "* *Ta có thể chạy thử dòng lệnh print(y_train[0]) và print(y_train_encode[0]) để kiểm tra kết quả*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa0tVhBydChx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# chuyển nhãn thành vector\n",
        "def category_vector(y):\n",
        "    vector = np.zeros(5)\n",
        "    vector[int(y)-1] = 1.0\n",
        "    return vector\n",
        "\n",
        "# Hàm chuyển toàn bộ nhãn y trong tập train và tập valid thành vector 5 chiều.\n",
        "def convert_y(y_data):\n",
        "    matrix = []\n",
        "    for item in y_data:\n",
        "        matrix.append(category_vector(item))\n",
        "    return matrix\n",
        "y_train_encode = convert_y(y_train)\n",
        "y_valid_encode = convert_y(y_valid)\n",
        "# Hiện thị ví dụ ở đây.\n",
        "# print(y_train[0]) --5.0\n",
        "# print(y_train_encode[0]) --[0. 0. 0. 0. 1.]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYYvDqZATMk2",
        "colab_type": "text"
      },
      "source": [
        "## II. SỬ DỤNG MÔ HÌNH HỒI QUY TUYẾN TÍNH DÙNG BÌNH PHƯƠNG TỐI TIỂU - XÂY DỰNG MÔ HÌNH M2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdVVucESTiT2",
        "colab_type": "text"
      },
      "source": [
        "* Bằng cách dùng mô hình hồi quy tuyến tính dùng bình phương tối tiểu, ta có thể xây dựng một mô hình phức tạp M2 dự đoán nhãn theo trường overall từ 1 - 5. Với mô hình này, kết quả của hồi quy tuyến tính một vector Y 5 chiều, index của chiều đại diện cho từng nhãn, thông qua hàm softmax để trả về kết quả phù hợp\n",
        "\n",
        "* Kết quả của hồi quy tuyến tính một vector Y 5 chiều, index của chiều đại diện cho từng nhãn đã được thực hiện ở phần trên\n",
        "\n",
        "* **Hàm linear_regression(X_train,y_train):** Với đầu vào là tập dữ liệu X_train, Y_train và đầu ra là giá trị theta. Hàm này xây dựng dựa trên mô hình hồi quy tuyến tính dùng bình phương tối tiểu\n",
        "\n",
        "* Ta có thể chạy thử lệnh print(\"theta : \", theta) để kiểm tra giá trị theta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NpNQQsXakNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hàm linear regression như ở lab 4. Đầu vào là x_train, y_train. đầu ra là giá trị theta\n",
        "def linear_regression(X_train,y_train):\n",
        "    theta = np.linalg.pinv(X_train) @ y_train\n",
        "    return theta\n",
        "# Huấn luyện trên dữ liệu X_train\n",
        "theta = linear_regression(histogram_X_train,y_train_encode)\n",
        "#print(\"theta : \", theta)\n",
        "#print(len(theta)) --6523\n",
        "\n",
        "# theta :  [[ 9.86483024e-12  5.51049173e-11 -2.29972476e-11 -3.01103150e-11\n",
        "#   -1.15498186e-11]\n",
        "#  [ 4.17777377e-02 -6.15464223e-02 -1.64461538e-01  2.43738281e-02\n",
        "#    1.15985639e+00]\n",
        "#  [ 1.68808765e+00  1.43154843e-01 -1.00180007e-01  3.50076476e+00\n",
        "#   -4.23182724e+00]\n",
        "#  ...\n",
        "#  [ 7.52603474e+00  9.98995941e+00 -9.06672293e+00  3.36612294e+01\n",
        "#   -4.08743587e+01]\n",
        "#  [-6.74111785e+00  2.03874042e+01  2.57644048e+01 -3.58669515e+01\n",
        "#   -2.94373973e+00]\n",
        "#  [-1.65997390e+00  3.36241697e-01  2.66890431e+00  1.05087079e+01\n",
        "#   -1.08538800e+01]]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WdPPmtUv7A",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm softmax(x):** Từ vector Y 5 chiều với index của chiều đại diện cho từng nhãn, thông qua hàm này để trả ra nhãn phù hợp. Công thức của softmax và chi tiết của nó ta có thể xem thêm [tại đây](https://machinelearningcoban.com/2017/02/17/softmax/) . Với đầu vào là một ma trận với mỗi cột là một vector x đầu ra cũng là một ma trận mà mỗi cột có giá trị là softmax(x). Các giá trị của x còn được gọi là scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_FWHiL2cy2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hàm softmax vì ở đây ta làm câu 5 nhãn (Mô hình M2. 10đ)\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvydD-snVq2m",
        "colab_type": "text"
      },
      "source": [
        "* **Hàm predict(theta, input):** Hàm này được dùng để dự đoán trên tập test\n",
        "  - **Tham số đầu vào:** theta, histogramX thứ i của tập valid \n",
        "  - **Tham số trả về:** nhãn dự đoán\n",
        "\n",
        "* Đoạn code dưới nữa còn được dùng để chạy dự đoán nhãn trên tập valid (tập test)\n",
        "và lưu vào danh sách. Ta có thể kiểm chứng bằng cách cho chạy dòng lệnh print(y_pred)*(nhãn dự đoán của mô hình linear)* và print(y_valid) *(nhãn đúng của nó)* để so sánh\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbWKqNx3bYRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sử dụng hàm softmax trước khi xác định nhãn.\n",
        "def predict(theta,input):\n",
        "    predict_value = np.dot(input, theta)\n",
        "    output = softmax(predict_value)\n",
        "    label = np.where(output == np.amax(output))[0][0] + 1\n",
        "    return label\n",
        "\n",
        "# Chạy dự đoán trên tập test và lưu vào danh sách.\n",
        "y_valid = [int(x) for x in y_valid]\n",
        "y_pred = []\n",
        "for index in range(len(histogram_X_valid)):\n",
        "    y_pred.append(predict(theta,histogram_X_valid[index]))\n",
        "# hàng ở trên là nhãn dự đoán của mô hình linear\n",
        "# Hàng ở dưới là nhãn đúng của nó.\n",
        "\n",
        "#print(y_pred) --[4, 5, 4, 4, 5, 5, 5, 1, 5, 4, 5, 4, 5, 5, 4, 5, 5, 4, 3, 5, 5, 5, 2, 3, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 3, 5, 2, 5, 5, 3, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 5, 4, 5, 5, 4, 5, 3, 5, 5, 5, 5, 3, 3, 5, 4, 5, 3, 5, 5, 4, 4, 5, 5, 3, 3, 1, 5, 1, 3, 5, 4, 5, 3, 4, 4, 5, 4, 5, 4, 5, 4, 5, 3, 4, 4, 5, 5, 4, 4, 5, 2, 4, 5, 3, 5, 5, 5, 1, 5, 5, 2, 5, 4, 5, 5, 5, 4, 2, 5, 5, 4, 5, 5, 5, 5, 4, 4, 5, 2, 5, 5, 5, 4, 4, 5, 4, 5, 5, 1, 1, 1, 4, 5, 5, 5, 3, 1, 5, 3, 2, 3, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 4, 5, 2, 5, 2, 5, 5, 5, 5, 5, 5, 4, 5, 4, 2, 5, 3, 1, 3, 5, 3, 3, 5, 4, 4, 5, 5, 1, 5, 5, 3, 5, 5, 5, 3, 3, 4, 4, 5, 5, 5, 3, 4, 2, 5, 4, 4, 3, 5, 1, 5, 4, 5, 4, 5, 2, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 4, 5, 5, 4, 5, 5, 5, 4, 5, 1, 4, 5, 5, 3, 5, 5, 4, 5, 4, 5, 5, 5, 4, 5, 4, 3, 5, 5, 5, 4, 5, 5, 5, 4, 5, 4, 4, 5, 5, 3, 4, 5, 5, 3, 5, 4, 5, 5, 2, 5, 5, 5, 4, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 4, 5, 3, 5, 1, 3, 5, 5, 3, 4, 2, 3, 5, 5, 5, 4, 5, 3, 5, 4, 5, 3, 5, 5, 2, 5, 4, 5, 5, 5, 5, 2, 5, 2, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 2, 5, 5, 1, 5, 5, 5, 5, 5, 4, 4, 3, 2, 5, 5, 5, 4, 4, 5, 5, 5, 3, 3, 3, 4, 2, 4, 5, 2, 2, 5, 5, 5, 4, 5, 5, 5, 5, 3, 4, 2, 1, 3, 5, 5, 5, 3, 5, 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 3, 4, 4, 5, 5, 4, 5, 5, 5, 5, 3, 3, 4, 5, 5, 3, 5, 5, 5, 5, 3, 4, 5, 5, 5, 4, 4, 5, 1, 5, 5, 5, 5, 5, 5, 3, 5, 2, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, 4, 5, 4, 5, 5, 4, 1, 1, 5, 5, 5, 5, 5, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 3, 5, 5, 3, 4, 5, 1, 3, 3, 3, 5, 1, 5, 3, 5, 5, 5, 5, 4, 4, 5, 5]\n",
        "#print(y_valid) -- [5, 3, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 1, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 5, 4, 5, 5, 3, 5, 5, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 1, 5, 4, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 3, 5, 3, 5, 5, 4, 5, 1, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 4, 5, 5, 3, 5, 5, 5, 5, 4, 5, 5, 5, 5, 2, 4, 5, 1, 1, 5, 5, 5, 5, 2, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 4, 3, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 2, 5, 2, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 4, 5, 4, 5, 5, 1, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 3, 2, 5, 5, 1, 1, 5, 5, 5, 5, 5, 4, 2, 5, 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 1, 4, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 3, 3, 3, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 1, 2, 5, 5, 3, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 4, 5, 5, 5, 5, 5, 1, 4, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 1, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 4, 5, 4, 1, 4, 4, 5, 5, 5, 5, 3, 1, 5, 5, 1, 5, 4, 4, 3, 4, 3, 5, 5, 5, 3, 5, 3, 5, 5, 5, 5, 5, 5]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDigtY_8TZNB",
        "colab_type": "text"
      },
      "source": [
        "## III. SỬ DỤNG ĐỘ CHÍNH XÁC ĐỂ ĐÁNH GIÁ MÔ HÌNH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCcX5HsfW0Bk",
        "colab_type": "text"
      },
      "source": [
        "### Mô hình được đánh giá bằng độ chính xác (accuracy)\n",
        "\n",
        "* **Hàm accuracy_score(y_pred,y_test):** Hàm này được dùng để tính độ chính xác của mô hình phân lớp dựa trên số nhãn dự đoán được, giá trị trả về sẽ * 100%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x5dGbf_f72D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc865996-0c84-4d99-bfc2-a7959c95ff52"
      },
      "source": [
        "# Tính độ chính xác * 100%.\n",
        "def accuracy_score(y_pred, y_test):\n",
        "    count = 0\n",
        "    for index,value in enumerate(y_test):\n",
        "        if value == y_pred[index]:\n",
        "           count +=1\n",
        "    acc = count/len(y_test)\n",
        "    return acc\n",
        "accuracy = accuracy_score(y_pred,y_valid)\n",
        "print(\"M2 - \", accuracy * 100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M2 -  51.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s-TGtceZIsc",
        "colab_type": "text"
      },
      "source": [
        "## IV. TÀI LIỆU THAM KHẢO\n",
        "\n",
        "1. https://machinelearningcoban.com/2017/02/17/softmax/\n",
        "2. Slide PDF hướng dẫn cho đồ án của cô Trần Thị Thảo Nhi\n",
        "3. https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
      ]
    }
  ]
}